#!/usr/bin/env python3
"""
Multi-GPU Competition Script - å¤šGPUç«äº‰è°ƒåº¦å™¨
Manages multi-GPU resource allocation based on available memory

æ¨¡å—ç»“æ„ï¼š
- utils/gpu_monitor.py: GPU çŠ¶æ€ç›‘æ§
- utils/process_json.py: JSON æ–‡ä»¶ç®¡ç†
- utils/retry.py: é‡è¯•æœºåˆ¶
- utils/gpus_command_file.py: å¤šGPUå‘½ä»¤æ–‡ä»¶è§£æ
- command_gpus.txt: å¤šGPUä»»åŠ¡é…ç½®æ–‡ä»¶

æ ¸å¿ƒç‰¹æ€§ï¼š
- æ”¯æŒä»»åŠ¡ä½¿ç”¨å¤šå¼ GPU
- ä¼˜å…ˆè°ƒåº¦GPUéœ€æ±‚é‡å¤§çš„ä»»åŠ¡
- å…¶æ¬¡ä¼˜å…ˆè°ƒåº¦é˜Ÿåˆ—IDå°çš„ä»»åŠ¡
"""

import os
import sys
import time
import subprocess
import logging
from typing import Dict, List, Optional, Set
from dataclasses import dataclass, field
import psutil

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, SCRIPT_DIR)

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils.gpu_monitor import GPUMonitor
from utils.process_json import ProcessJSON
from utils.retry import RetryConfig, is_task_ready, handle_task_retry, generate_uni_id
from utils.gpus_command_file import parse_command_file
from utils.process_yaml import ProcessYAML
from utils.gpu_select import GPUSelector, select_gpus

# è¿è¡Œæ–¹å¼:
# nohup python main_gpus.py > /dev/null 2>&1 &

# =============================================================================
# é…ç½®é¡¹
# =============================================================================

# è·¯å¾„é…ç½®ï¼ˆç›¸å¯¹äºè„šæœ¬ä½ç½®ï¼‰
log_dir = os.path.join(SCRIPT_DIR, 'logs')
process_json_path = os.path.join(SCRIPT_DIR, 'logs', 'uni_id_gpus.json')  # å¤šGPUä¸“ç”¨JSON
config_path = os.path.join(SCRIPT_DIR, 'config.yaml')   # YAML é…ç½®æ–‡ä»¶è·¯å¾„

# åŠ è½½é…ç½®
config_processor = ProcessYAML(config_path)
config = config_processor.get_config()

# å·¥ä½œç›®å½•é…ç½®ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
work_dir_config = config.get('work_dir')
if work_dir_config:
    # å¦‚æœé…ç½®äº† work_dirï¼Œè§£æä¸ºç»å¯¹è·¯å¾„
    if os.path.isabs(work_dir_config):
        work_dir = work_dir_config
    else:
        # ç›¸å¯¹è·¯å¾„ï¼šç›¸å¯¹äºè„šæœ¬ä½ç½®
        work_dir = os.path.abspath(os.path.join(SCRIPT_DIR, work_dir_config))
else:
    # é»˜è®¤ï¼šè„šæœ¬çˆ¶ç›®å½•
    work_dir = os.path.dirname(SCRIPT_DIR)

# è°ƒåº¦é…ç½®
check_time = config.get('check_time', 5)  # è°ƒåº¦é—´éš”ï¼ˆç§’ï¼‰
maximize_resource_utilization = config.get('maximize_resource_utilization', False)  # æé™åˆ©ç”¨èµ„æºæ¨¡å¼
memory_save_mode = config.get('memory_save_mode', True)  # GPUé€‰æ‹©æ¨¡å¼ï¼šTrue=èŠ‚çœæ˜¾å­˜ï¼ŒFalse=é˜²æ­¢æº¢å‡º

# GPU é…ç½®
compete_gpus = config.get('compete_gpus', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  # æ‰‹åŠ¨æŒ‡å®šçš„ GPU åˆ—è¡¨
use_all_gpus = config.get('use_all_gpus', True)  # æ˜¯å¦è‡ªåŠ¨æ¢æµ‹æ‰€æœ‰ GPU
gpu_left = config.get('gpu_left', 0)  # å‰©ä½™å‡ å¼ å¡ç»™å…¶ä»–ç”¨æˆ·
min_gpu = config.get('min_gpu', 3)  # ç”¨æˆ·è‡³å°‘ç”¨å‡ å¼ å¡
max_gpu = config.get('max_gpu', 8)  # ç”¨æˆ·æœ€å¤šç”¨å‡ å¼ å¡

# é‡è¯•é…ç½®
retry_config_dict = config.get('retry_config', {})
retry_config = RetryConfig(
    max_retry_before_backoff=retry_config_dict.get('max_retry_before_backoff', 3),  # æ¯ 3 æ¬¡é‡è¯•åè¿›å…¥é€€é¿
    backoff_duration=retry_config_dict.get('backoff_duration', 600)         # é€€é¿æ—¶é—´ 10 åˆ†é’Ÿ
)

# å‘½ä»¤æ–‡ä»¶é…ç½®
gpus_command_file = config.get('gpus_command_file', 'command_gpus.txt')
commands_path = os.path.join(SCRIPT_DIR, gpus_command_file)


# =============================================================================
# ä»»åŠ¡æ•°æ®ç»“æ„ï¼ˆå¤šGPUç‰ˆæœ¬ï¼‰
# =============================================================================

@dataclass
class Task:
    """å¤šGPUä»»åŠ¡æ•°æ®ç»“æ„"""
    commands: List[str]          # å‘½ä»¤åˆ—è¡¨ï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰
    queue_id: int                # é˜Ÿåˆ— ID
    gpu_count: int               # GPU æ•°é‡éœ€æ±‚
    estimated_memory_gb: int     # æ¯å¼ GPUé¢„ä¼°æ˜¾å­˜ (GB)
    uni_id: str = ""             # å”¯ä¸€æ ‡è¯†ç¬¦
    status: str = "pending"      # pending / running / completed / failed
    assigned_gpus: List[int] = field(default_factory=list)  # åˆ†é…çš„ GPU ID åˆ—è¡¨
    pid: int = 0                 # Python è¿›ç¨‹ PID
    retry_count: int = 0         # é‡è¯•æ¬¡æ•°
    backoff_until: float = 0     # é€€é¿ç»“æŸæ—¶é—´æˆ³
    error_type: str = ""         # é”™è¯¯ç±»å‹


class MultiGPUCompetitor:
    """å¤šGPUç«äº‰è°ƒåº¦å™¨ - æ ¸å¿ƒç±»
    
    æ ¸å¿ƒé€»è¾‘ï¼šé˜Ÿåˆ—å†…ä¸²è¡Œï¼Œé˜Ÿåˆ—é—´å¹¶è¡Œï¼Œä¼˜å…ˆå¤§ä»»åŠ¡
    - åŒä¸€é˜Ÿåˆ—çš„ä»»åŠ¡ä¸¥æ ¼æŒ‰é¡ºåºæ‰§è¡Œ
    - ä¸åŒé˜Ÿåˆ—çš„ä»»åŠ¡å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼ˆåœ¨ä¸åŒ GPU ä¸Šï¼‰
    - ä¼˜å…ˆè°ƒåº¦GPUéœ€æ±‚é‡å¤§çš„ä»»åŠ¡
    - å…¶æ¬¡ä¼˜å…ˆè°ƒåº¦é˜Ÿåˆ—IDå°çš„ä»»åŠ¡
    - æ¯æ¬¡åªåˆ†é…ä¸€ä¸ªä»»åŠ¡ï¼Œç­‰å¾…ç¡®è®¤åå†åˆ†é…ä¸‹ä¸€ä¸ª
    """
    
    def __init__(self):
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ– GPU åˆ—è¡¨
        if use_all_gpus:
            all_gpus = GPUMonitor.detect_gpus()
        else:
            all_gpus = compete_gpus
        
        # è®¡ç®—å¯ç”¨ GPU æ•°é‡ï¼šmin(max_gpu, max(min_gpu, available_gpus - gpu_left))
        available_after_reservation = len(all_gpus) - gpu_left if len(all_gpus) > gpu_left else 0
        min_required = max(min_gpu, available_after_reservation)
        target_gpu_count = min(max_gpu, min_required)
        
        # ç¡®ä¿ä¸è¶…è¿‡å®é™…å¯ç”¨çš„ GPU æ•°é‡
        target_gpu_count = min(target_gpu_count, len(all_gpus))
        
        # åº”ç”¨ GPU åˆ†é…é€»è¾‘
        if target_gpu_count < len(all_gpus):
            # ä»å‰é¢å– target_gpu_count å¼ å¡
            self.gpus = all_gpus[:target_gpu_count]
            reserved_gpus = all_gpus[target_gpu_count:]
            # è®°å½•é¢„ç•™çš„ GPUï¼ˆåŒ…æ‹¬ gpu_left å’Œå¤šä½™çš„ï¼‰
            logging.info(f"ğŸ–¥ï¸ Using {len(self.gpus)}/{len(all_gpus)} GPUs: {self.gpus}")
            logging.info(f"ğŸ–¥ï¸ Reserved GPUs: {reserved_gpus} (gpu_left={gpu_left}, excess={len(reserved_gpus)-gpu_left})")
        else:
            self.gpus = all_gpus
            logging.info(f"ğŸ–¥ï¸ Using all {len(self.gpus)} GPUs: {self.gpus}")
        
        # ä¿å­˜é…ç½®åˆ°å®ä¾‹å˜é‡
        self.gpu_left = gpu_left
        self.min_gpu = min_gpu
        self.max_gpu = max_gpu
        self.total_gpus = len(all_gpus)
        
        # åˆå§‹åŒ– JSON ç®¡ç†å™¨
        self.process_json = ProcessJSON(process_json_path)
        
        # åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
        self.tasks: List[Task] = []
        self.queues: Dict[int, List[Task]] = {}  # queue_id -> [tasks]
        self._setup_tasks()
        
        # è¿è¡ŒçŠ¶æ€
        self.running = True
        
        # é…ç½®
        self.task_start_delay = 30  # æ¯ä¸ªä»»åŠ¡å¯åŠ¨åç­‰å¾…ç§’æ•°
    
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        os.makedirs(log_dir, exist_ok=True)
        log_file = self._get_next_log_file()
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        logging.info(f"ğŸ“ Log file: {log_file}")
    
    def _get_next_log_file(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªæ—¥å¿—æ–‡ä»¶å"""
        base = os.path.join(log_dir, 'compete_gpus')  # å¤šGPUä¸“ç”¨æ—¥å¿—
        if not os.path.exists(f"{base}.log"):
            return f"{base}.log"
        i = 1
        while os.path.exists(f"{base}({i}).log"):
            i += 1
        return f"{base}({i}).log"
    
    def _setup_tasks(self):
        """ä»å‘½ä»¤æ–‡ä»¶åˆå§‹åŒ–ä»»åŠ¡åˆ—è¡¨ï¼ˆå¤šGPUç‰ˆæœ¬ï¼‰"""
        command_tasks = parse_command_file(commands_path)
        
        if not command_tasks:
            logging.warning(f"No tasks found in {commands_path}")
            return
        
        for commands, queue_id, gpu_count, memory in command_tasks:
            task = Task(
                commands=commands,
                queue_id=queue_id,
                gpu_count=gpu_count,
                estimated_memory_gb=memory,
                uni_id=generate_uni_id()
            )
            self.tasks.append(task)
            
            if queue_id not in self.queues:
                self.queues[queue_id] = []
            self.queues[queue_id].append(task)
        
        logging.info(f"ğŸ“‹ Total tasks: {len(self.tasks)}, Queues: {list(self.queues.keys())}")
        for qid, tasks in self.queues.items():
            gpu_counts = [t.gpu_count for t in tasks]
            logging.info(f"   Queue {qid}: {len(tasks)} tasks, GPU needs: {gpu_counts}")
    
    def find_available_gpus(self, gpu_count: int, required_memory: int, exclude_gpus: Set[int] = None) -> Optional[List[int]]:
        """æŸ¥æ‰¾å¤šä¸ªå¯ç”¨çš„ GPU
        
        æ¡ä»¶ï¼š
        1. æœ‰è¶³å¤Ÿçš„æ˜¾å­˜
        2. éæé™æ¨¡å¼ä¸‹ï¼Œå½“å‰ç”¨æˆ·æ²¡æœ‰å…¶ä»– Python è¿›ç¨‹åœ¨è¯¥ GPU ä¸Š
        3. å¤šä¸ªå¯ç”¨GPUæ—¶ï¼Œä½¿ç”¨æ™ºèƒ½é€‰æ‹©ç­–ç•¥
        
        Args:
            gpu_count: éœ€è¦çš„ GPU æ•°é‡
            required_memory: æ¯å¼  GPU éœ€è¦çš„æ˜¾å­˜ (GB)
            exclude_gpus: æ’é™¤çš„ GPU é›†åˆ
            
        Returns:
            å¯ç”¨çš„ GPU ID åˆ—è¡¨ï¼Œå¦‚æœä¸è¶³åˆ™è¿”å› None
        """
        exclude_gpus = exclude_gpus or set()
        
        # ç¬¬ä¸€æ­¥ï¼šç­›é€‰å‡ºæ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„GPU
        candidate_gpus = []
        for gpu_id in self.gpus:
            if gpu_id in exclude_gpus:
                continue
            
            # æ£€æŸ¥æ˜¾å­˜
            available = GPUMonitor.get_available_memory(gpu_id)
            if available < required_memory:
                logging.debug(f"GPU {gpu_id}: insufficient memory ({available:.1f}GB < {required_memory}GB)")
                continue
            
            # éæé™æ¨¡å¼ï¼šæ£€æŸ¥ç”¨æˆ·è¿›ç¨‹
            if not maximize_resource_utilization:
                user_procs = GPUMonitor.get_user_processes_on_gpu(gpu_id)
                if user_procs:
                    logging.debug(f"GPU {gpu_id}: user processes exist {user_procs}")
                    continue
            
            candidate_gpus.append(gpu_id)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å€™é€‰GPU
        if len(candidate_gpus) < gpu_count:
            logging.debug(f"Only found {len(candidate_gpus)} GPUs, need {gpu_count}")
            return None
        
        # ç¬¬äºŒæ­¥ï¼šå¦‚æœå€™é€‰GPUæ•°é‡åˆšå¥½ç­‰äºéœ€æ±‚ï¼Œç›´æ¥è¿”å›
        if len(candidate_gpus) == gpu_count:
            logging.info(f"âœ… Found exactly {gpu_count} GPUs: {candidate_gpus}")
            return candidate_gpus
        
        # ç¬¬ä¸‰æ­¥ï¼šå¤šä¸ªå€™é€‰GPUæ—¶ï¼Œä½¿ç”¨æ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼ˆé«˜é¢‘é‡‡æ ·ï¼‰
        logging.info(f"ğŸ” å‘ç° {len(candidate_gpus)} ä¸ªå€™é€‰GPU: {candidate_gpus}ï¼Œéœ€è¦ {gpu_count} ä¸ªï¼Œå¯åŠ¨æ™ºèƒ½é€‰æ‹©...")
        
        best_gpus = select_gpus(
            gpu_ids=candidate_gpus,
            count=gpu_count,
            memory_save_mode=memory_save_mode,
            required_memory=required_memory,
            use_sampling=True  # ä½¿ç”¨3ç§’30æ¬¡é‡‡æ ·
        )
        
        if len(best_gpus) >= gpu_count:
            return best_gpus[:gpu_count]
        
        # å¦‚æœæ™ºèƒ½é€‰æ‹©è¿”å›çš„GPUä¸è¶³ï¼Œå›é€€åˆ°å‰Nä¸ªå€™é€‰GPU
        logging.warning(f"æ™ºèƒ½é€‰æ‹©è¿”å› {len(best_gpus)} ä¸ªGPUï¼Œä¸è¶³ {gpu_count} ä¸ªï¼Œå›é€€åˆ°å€™é€‰åˆ—è¡¨")
        return candidate_gpus[:gpu_count]
    
    def get_busy_queues(self) -> set:
        """è·å–å½“å‰æ­£åœ¨è¿è¡Œä»»åŠ¡çš„é˜Ÿåˆ— ID é›†åˆ
        
        é€šè¿‡æ£€æŸ¥ JSON ä¸­ state=running ä¸”è¿›ç¨‹ç¡®å®å­˜åœ¨çš„è®°å½•
        """
        busy = set()
        running = self.process_json.get_running_processes()
        
        for uni_id, record in running.items():
            pid = record.get('pid', 0)
            if pid > 0 and psutil.pid_exists(pid):
                # æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡è·å–é˜Ÿåˆ— ID
                for task in self.tasks:
                    if task.uni_id == uni_id:
                        busy.add(task.queue_id)
                        break
        
        return busy
    
    def get_occupied_gpus(self) -> set:
        """è·å–å½“å‰è¢«å ç”¨çš„ GPU é›†åˆï¼ˆéæé™æ¨¡å¼ä¸‹ï¼‰"""
        if maximize_resource_utilization:
            return set()
        
        occupied = set()
        for gpu_id in self.gpus:
            user_procs = GPUMonitor.get_user_processes_on_gpu(gpu_id)
            if user_procs:
                occupied.add(gpu_id)
        return occupied
    
    def get_queue_head_task(self, queue_id: int) -> Optional[Task]:
        """è·å–é˜Ÿåˆ—çš„ç¬¬ä¸€ä¸ª pending ä»»åŠ¡"""
        for task in self.queues.get(queue_id, []):
            if task.status == "pending":
                return task
        return None
    
    def execute_task(self, task: Task, gpu_ids: List[int]) -> bool:
        """æ‰§è¡Œå¤šGPUä»»åŠ¡ï¼ˆåŒæ­¥æ‰§è¡Œæ‰€æœ‰å‘½ä»¤ï¼‰
        
        Args:
            task: ä»»åŠ¡å¯¹è±¡
            gpu_ids: åˆ†é…çš„ GPU ID åˆ—è¡¨
            
        Returns:
            True å¦‚æœä»»åŠ¡æˆåŠŸå¯åŠ¨ï¼ˆbash è„šæœ¬å·²å¯åŠ¨åå°è¿›ç¨‹ï¼‰
        """
        task.assigned_gpus = gpu_ids
        task.status = "running"
        
        # æ„å»º CUDA_VISIBLE_DEVICES å­—ç¬¦ä¸²
        cuda_devices = ','.join(map(str, gpu_ids))
        
        logging.info(f"ğŸš€ Starting task {task.uni_id} (Queue {task.queue_id}) on GPUs {gpu_ids}")
        
        for i, cmd_template in enumerate(task.commands):
            # æ›¿æ¢å˜é‡
            cmd = cmd_template.format(
                work_dir=work_dir,
                uni_id=task.uni_id
            )
            
            # åœ¨å‘½ä»¤å‰æ·»åŠ  CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿å­è¿›ç¨‹ç»§æ‰¿ï¼‰
            full_cmd = f"CUDA_VISIBLE_DEVICES={cuda_devices} {cmd}"
            
            logging.info(f"   [{i+1}/{len(task.commands)}] [GPUs {cuda_devices}] {cmd[:80]}...")
            
            try:
                result = subprocess.run(
                    full_cmd, shell=True, capture_output=True, text=True, timeout=300
                )
                
                if result.returncode != 0:
                    logging.error(f"   Command failed: {result.stderr[:200]}")
                    task.status = "failed"
                    return False
                
                # æ‰“å°è¾“å‡ºï¼ˆç®€åŒ–ï¼‰
                if result.stdout.strip():
                    for line in result.stdout.strip().split('\n')[:5]:
                        logging.info(f"   > {line[:100]}")
                        
            except subprocess.TimeoutExpired:
                logging.error(f"   Command timeout")
                task.status = "failed"
                return False
            except Exception as e:
                logging.error(f"   Command error: {e}")
                task.status = "failed"
                return False
        
        task.status = "completed"
        logging.info(f"âœ… Task {task.uni_id} commands completed, waiting for background process...")
        return True
    
    def wait_for_process_start(self, task: Task, timeout: int = 60) -> bool:
        """ç­‰å¾…ä»»åŠ¡çš„ Python è¿›ç¨‹çœŸæ­£å¯åŠ¨
        
        é€šè¿‡æ£€æŸ¥ JSON ä¸­çš„ PID æ˜¯å¦æœ‰æ•ˆæ¥ç¡®è®¤
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            record = self.process_json.get_record(task.uni_id)
            if record:
                pid = record.get('pid', 0)
                state = record.get('state', '')
                
                if state == 'running' and pid > 0 and psutil.pid_exists(pid):
                    task.pid = pid
                    logging.info(f"âœ… Process confirmed: {task.uni_id} PID={pid}")
                    return True
                elif state in ('normal_exit', 'abnormal_exit'):
                    logging.warning(f"âš ï¸ Process already exited: {task.uni_id} state={state}")
                    return True  # è¿›ç¨‹å·²ç»“æŸï¼Œä¹Ÿç®—ç¡®è®¤
            
            time.sleep(2)
        
        logging.warning(f"â° Timeout waiting for process: {task.uni_id}")
        return False
    
    def print_status(self):
        """æ‰“å°å½“å‰çŠ¶æ€"""
        pending = sum(1 for t in self.tasks if t.status == "pending")
        running = sum(1 for t in self.tasks if t.status == "running")
        completed = sum(1 for t in self.tasks if t.status == "completed")
        failed = sum(1 for t in self.tasks if t.status == "failed")
        
        logging.info("=" * 60)
        logging.info(f"ğŸ“Š Tasks: Pending={pending}, Running={running}, Completed={completed}, Failed={failed}")
        
        busy_queues = self.get_busy_queues()
        for qid in sorted(self.queues.keys()):
            status = "ğŸ”´ BUSY" if qid in busy_queues else "ğŸŸ¢ IDLE"
            q_pending = sum(1 for t in self.queues[qid] if t.status == "pending")
            q_completed = sum(1 for t in self.queues[qid] if t.status == "completed")
            logging.info(f"   Queue {qid}: {status}, Pending={q_pending}, Completed={q_completed}")
        
        logging.info("=" * 60)
    
    def check_and_handle_finished_tasks(self):
        """æ£€æŸ¥å·²å®Œæˆ/å¼‚å¸¸çš„ä»»åŠ¡å¹¶å¤„ç†é‡è¯•
        
        é€šè¿‡ JSON æ–‡ä»¶æ£€æµ‹è¿›ç¨‹çŠ¶æ€å˜åŒ–
        """
        for task in self.tasks:
            if task.status != "running" and task.status != "completed":
                continue
            
            record = self.process_json.get_record(task.uni_id)
            if not record:
                continue
            
            state = record.get('state', '')
            
            if state == 'normal_exit':
                # æ­£å¸¸é€€å‡º
                if task.status != "completed":
                    task.status = "completed"
                    logging.info(f"âœ… Task {task.uni_id} (Queue {task.queue_id}) completed successfully")
            
            elif state == 'abnormal_exit':
                # å¼‚å¸¸é€€å‡ºï¼Œéœ€è¦é‡è¯•
                if task.status == "running":
                    error_type = record.get('error_type', 'unknown')
                    handle_task_retry(task, error_type, retry_config)
    
    def _sort_candidate_tasks(self, tasks: List[Task]) -> List[Task]:
        """å¯¹å€™é€‰ä»»åŠ¡è¿›è¡Œæ’åº
        
        æ’åºè§„åˆ™ï¼š
        1. ä¼˜å…ˆGPUéœ€æ±‚é‡å¤§çš„ä»»åŠ¡
        2. å…¶æ¬¡ä¼˜å…ˆé˜Ÿåˆ—IDå°çš„ä»»åŠ¡
        """
        return sorted(tasks, key=lambda t: (-t.gpu_count, t.queue_id))
    
    def run(self):
        """ä¸»è°ƒåº¦å¾ªç¯ï¼ˆå¤šGPUç‰ˆæœ¬ï¼‰
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. æ£€æŸ¥å·²å®Œæˆ/å¼‚å¸¸çš„ä»»åŠ¡ï¼Œå¤„ç†é‡è¯•
        2. è·å–æ‰€æœ‰ç©ºé—²é˜Ÿåˆ—çš„å¤´éƒ¨ä»»åŠ¡
        3. æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆGPUéœ€æ±‚é‡å¤§ä¼˜å…ˆï¼Œé˜Ÿåˆ—IDå°ä¼˜å…ˆï¼‰
        4. é€ä¸ªåˆ†é…ä»»åŠ¡åˆ°å¯ç”¨ GPU
        5. æ¯åˆ†é…ä¸€ä¸ªä»»åŠ¡åç­‰å¾…ç¡®è®¤ï¼Œå†åˆ†é…ä¸‹ä¸€ä¸ª
        """
        logging.info("ğŸ Starting Multi-GPU competition scheduler")
        self.print_status()
        
        try:
            while self.running:
                # æ£€æŸ¥å·²å®Œæˆ/å¼‚å¸¸çš„ä»»åŠ¡
                self.check_and_handle_finished_tasks()
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆ
                incomplete_tasks = [t for t in self.tasks if t.status != "completed"]
                if not incomplete_tasks:
                    logging.info("ğŸ‰ All tasks completed!")
                    break
                
                # è·å–å½“å‰å¿™ç¢Œçš„é˜Ÿåˆ—
                busy_queues = self.get_busy_queues()
                
                # è·å–ç©ºé—²é˜Ÿåˆ—çš„å¤´éƒ¨ä»»åŠ¡ï¼ˆè€ƒè™‘é€€é¿ï¼‰
                candidate_tasks = []
                for qid in self.queues.keys():
                    if qid not in busy_queues:
                        head_task = self.get_queue_head_task(qid)
                        if head_task and is_task_ready(head_task):
                            candidate_tasks.append(head_task)
                
                if not candidate_tasks:
                    # æ²¡æœ‰å¯è°ƒåº¦çš„ä»»åŠ¡ï¼Œç­‰å¾…
                    time.sleep(check_time)
                    continue
                
                # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šGPUéœ€æ±‚é‡å¤§ä¼˜å…ˆï¼Œé˜Ÿåˆ—IDå°ä¼˜å…ˆ
                candidate_tasks = self._sort_candidate_tasks(candidate_tasks)
                logging.info(f"ğŸ“‹ Candidate tasks (sorted): {[(t.queue_id, t.gpu_count) for t in candidate_tasks]}")
                
                # è·å–å½“å‰è¢«å ç”¨çš„ GPU
                occupied_gpus = self.get_occupied_gpus()
                
                # é€ä¸ªåˆ†é…ä»»åŠ¡
                tasks_started = 0
                for task in candidate_tasks:
                    # é‡æ–°è·å–å ç”¨çš„ GPUï¼ˆå®æ—¶æ£€æµ‹ï¼‰
                    occupied_gpus = self.get_occupied_gpus()
                    
                    # æŸ¥æ‰¾è¶³å¤Ÿæ•°é‡çš„å¯ç”¨ GPU
                    gpu_ids = self.find_available_gpus(task.gpu_count, task.estimated_memory_gb, occupied_gpus)
                    
                    if gpu_ids is None:
                        logging.info(f"â³ Not enough GPUs for task {task.uni_id} (need {task.gpu_count} GPUs, {task.estimated_memory_gb}GB each)")
                        continue
                    
                    # æ‰§è¡Œä»»åŠ¡
                    success = self.execute_task(task, gpu_ids)
                    
                    if success:
                        tasks_started += 1
                        
                        # ç­‰å¾…è¿›ç¨‹ç¡®è®¤å¯åŠ¨
                        self.wait_for_process_start(task, timeout=60)
                        
                        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†åˆ†é…ä¸‹ä¸€ä¸ªä»»åŠ¡
                        logging.info(f"â³ Waiting {self.task_start_delay}s before next task...")
                        time.sleep(self.task_start_delay)
                
                if tasks_started > 0:
                    logging.info(f"ğŸ“ˆ Started {tasks_started} task(s) this round")
                
                # æ‰“å°çŠ¶æ€
                self.print_status()
                
                # ç­‰å¾…ä¸‹ä¸€è½®è°ƒåº¦
                time.sleep(check_time)
                
        except KeyboardInterrupt:
            logging.info("ğŸ›‘ Interrupted by user")
            self.running = False
        
        logging.info("ğŸ Scheduler stopped")


# =============================================================================
# ä¸»å…¥å£
# =============================================================================

if __name__ == "__main__":
    competitor = MultiGPUCompetitor()
    competitor.run()
