#!/usr/bin/env python3
"""
GPU å†…å­˜å ç”¨æµ‹è¯•è„šæœ¬
- æ¶ˆè€—çº¦500MBæ˜¾å­˜
- è¿è¡Œçº¦60ç§’
- ç”¨äºæµ‹è¯•GPUè°ƒåº¦å™¨åŠŸèƒ½
"""

import torch
import time
import sys
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_gpu_memory(duration_seconds=60, memory_mb=500):
    """
    æµ‹è¯•GPUå†…å­˜å ç”¨
    
    Args:
        duration_seconds: è¿è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰
        memory_mb: æ˜¾å­˜å ç”¨ï¼ˆMBï¼‰
    """
    try:
        # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
        if not torch.cuda.is_available():
            logger.error("âŒ CUDA not available")
            return False
        
        device = torch.device('cuda')
        logger.info(f"âœ… Using GPU: {torch.cuda.get_device_name(0)}")
        
        # è®¡ç®—éœ€è¦åˆ†é…çš„å¼ é‡å¤§å°ï¼ˆMB -> å…ƒç´ æ•°ï¼‰
        # float32 = 4å­—èŠ‚ï¼Œæ‰€ä»¥ memory_mb * 1024 * 1024 / 4 = å…ƒç´ æ•°
        num_elements = (memory_mb * 1024 * 1024) // 4
        
        logger.info(f"ğŸš€ Allocating ~{memory_mb}MB GPU memory...")
        
        # åˆ†é…æ˜¾å­˜
        tensor = torch.randn(num_elements, dtype=torch.float32, device=device)
        
        # è·å–å®é™…åˆ†é…çš„æ˜¾å­˜
        torch.cuda.synchronize()
        allocated = torch.cuda.memory_allocated() / (1024 * 1024)
        logger.info(f"âœ… Allocated: {allocated:.1f}MB")
        
        # æ‰§è¡Œä¸€äº›è®¡ç®—ä»¥ä¿æŒGPUæ´»è·ƒ
        logger.info(f"â±ï¸ Running for {duration_seconds} seconds...")
        start_time = time.time()
        iteration = 0
        
        while time.time() - start_time < duration_seconds:
            # æ‰§è¡Œç®€å•çš„çŸ©é˜µè¿ç®—
            _ = torch.matmul(tensor[:1000], tensor[:1000].T)
            iteration += 1
            
            # æ¯10ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
            elapsed = time.time() - start_time
            if elapsed % 10 < 1 and iteration % 100 == 0:
                logger.info(f"   Progress: {elapsed:.1f}s / {duration_seconds}s (iteration: {iteration})")
            
            time.sleep(0.1)
        
        logger.info(f"âœ… Test completed successfully!")
        logger.info(f"   Total iterations: {iteration}")
        logger.info(f"   Total time: {time.time() - start_time:.1f}s")
        
        # æ¸…ç†
        del tensor
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–é…ç½®
    duration = int(sys.argv[1]) if len(sys.argv) > 1 else 60
    memory = int(sys.argv[2]) if len(sys.argv) > 2 else 500
    
    logger.info(f"GPU Memory Test: duration={duration}s, memory={memory}MB")
    
    success = test_gpu_memory(duration_seconds=duration, memory_mb=memory)
    sys.exit(0 if success else 1)
