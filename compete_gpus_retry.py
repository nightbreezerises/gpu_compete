#!/usr/bin/env python3
"""
GPU Competition Script - æ¨¡å—åŒ–ç‰ˆæœ¬
Manages GPU resource allocation based on available memory

æ¨¡å—ç»“æ„ï¼š
- utils/gpu_monitor.py: GPU çŠ¶æ€ç›‘æ§
- utils/process_json.py: JSON æ–‡ä»¶ç®¡ç†
- utils/retry.py: é‡è¯•æœºåˆ¶
- command.txt: ä»»åŠ¡é…ç½®æ–‡ä»¶
"""

import os
import sys
import time
import subprocess
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass
import psutil
import random

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, SCRIPT_DIR)

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils.gpu_monitor import GPUMonitor
from utils.process_json import ProcessJSON
from utils.retry import RetryConfig, is_task_ready, handle_task_retry, generate_uni_id
from utils.parse_command_file import parse_command_file
from utils.process_yaml import ProcessYAML

# è¿è¡Œæ–¹å¼:
# nohup python compete_gpus_retry.py > /dev/null 2>&1 &

# =============================================================================
# é…ç½®é¡¹
# =============================================================================

# è·¯å¾„é…ç½®ï¼ˆç›¸å¯¹äºè„šæœ¬ä½ç½®ï¼‰
log_dir = os.path.join(SCRIPT_DIR, 'logs')
process_json_path = os.path.join(SCRIPT_DIR, 'logs', 'uni_id.json')
commands_path = os.path.join(SCRIPT_DIR, 'command.txt')  # å‘½ä»¤é…ç½®æ–‡ä»¶è·¯å¾„
config_path = os.path.join(SCRIPT_DIR, 'config.yaml')   # YAML é…ç½®æ–‡ä»¶è·¯å¾„

# åŠ è½½é…ç½®
config_processor = ProcessYAML(config_path)
config = config_processor.get_config()

# å·¥ä½œç›®å½•é…ç½®ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
work_dir_config = config.get('work_dir')
if work_dir_config:
    # å¦‚æœé…ç½®äº† work_dirï¼Œè§£æä¸ºç»å¯¹è·¯å¾„
    if os.path.isabs(work_dir_config):
        work_dir = work_dir_config
    else:
        # ç›¸å¯¹è·¯å¾„ï¼šç›¸å¯¹äºè„šæœ¬ä½ç½®
        work_dir = os.path.abspath(os.path.join(SCRIPT_DIR, work_dir_config))
else:
    # é»˜è®¤ï¼šè„šæœ¬çˆ¶ç›®å½•
    work_dir = os.path.dirname(SCRIPT_DIR)

# è°ƒåº¦é…ç½®
check_time = config.get('check_time', 5)  # è°ƒåº¦é—´éš”ï¼ˆç§’ï¼‰
maximize_resource_utilization = config.get('maximize_resource_utilization', False)  # æé™åˆ©ç”¨èµ„æºæ¨¡å¼

# GPU é…ç½®
compete_gpus = config.get('compete_gpus', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  # æ‰‹åŠ¨æŒ‡å®šçš„ GPU åˆ—è¡¨
use_all_gpus = config.get('use_all_gpus', True)  # æ˜¯å¦è‡ªåŠ¨æ¢æµ‹æ‰€æœ‰ GPU

# é‡è¯•é…ç½®
retry_config_dict = config.get('retry_config', {})
retry_config = RetryConfig(
    max_retry_before_backoff=retry_config_dict.get('max_retry_before_backoff', 3),  # æ¯ 3 æ¬¡é‡è¯•åè¿›å…¥é€€é¿
    backoff_duration=retry_config_dict.get('backoff_duration', 600)         # é€€é¿æ—¶é—´ 10 åˆ†é’Ÿ
)


# =============================================================================
# å‘½ä»¤æ–‡ä»¶è§£æï¼ˆå·²ç§»è‡³ utils/parse_command_file.pyï¼‰
# =============================================================================


# =============================================================================
# ä»»åŠ¡æ•°æ®ç»“æ„
# =============================================================================

@dataclass
class Task:
    """ä»»åŠ¡æ•°æ®ç»“æ„"""
    commands: List[str]          # å‘½ä»¤åˆ—è¡¨ï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰
    queue_id: int                # é˜Ÿåˆ— ID
    estimated_memory_gb: int     # é¢„ä¼°æ˜¾å­˜ (GB)
    uni_id: str = ""             # å”¯ä¸€æ ‡è¯†ç¬¦
    status: str = "pending"      # pending / running / completed / failed
    assigned_gpu: int = -1       # åˆ†é…çš„ GPU ID
    pid: int = 0                 # Python è¿›ç¨‹ PID
    retry_count: int = 0         # é‡è¯•æ¬¡æ•°
    backoff_until: float = 0     # é€€é¿ç»“æŸæ—¶é—´æˆ³
    error_type: str = ""         # é”™è¯¯ç±»å‹


class GPUCompetitor:
    """GPU ç«äº‰è°ƒåº¦å™¨ - æ ¸å¿ƒç±»
    
    æ ¸å¿ƒé€»è¾‘ï¼šé˜Ÿåˆ—å†…ä¸²è¡Œï¼Œé˜Ÿåˆ—é—´å¹¶è¡Œ
    - åŒä¸€é˜Ÿåˆ—çš„ä»»åŠ¡ä¸¥æ ¼æŒ‰é¡ºåºæ‰§è¡Œ
    - ä¸åŒé˜Ÿåˆ—çš„ä»»åŠ¡å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼ˆåœ¨ä¸åŒ GPU ä¸Šï¼‰
    - æ¯æ¬¡åªåˆ†é…ä¸€ä¸ªä»»åŠ¡ï¼Œç­‰å¾…ç¡®è®¤åå†åˆ†é…ä¸‹ä¸€ä¸ª
    """
    
    def __init__(self):
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ– GPU åˆ—è¡¨
        if use_all_gpus:
            self.gpus = GPUMonitor.detect_gpus()
        else:
            self.gpus = compete_gpus
        logging.info(f"ğŸ–¥ï¸ Available GPUs: {self.gpus}")
        
        # åˆå§‹åŒ– JSON ç®¡ç†å™¨
        self.process_json = ProcessJSON(process_json_path)
        
        # åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
        self.tasks: List[Task] = []
        self.queues: Dict[int, List[Task]] = {}  # queue_id -> [tasks]
        self._setup_tasks()
        
        # è¿è¡ŒçŠ¶æ€
        self.running = True
        
        # é…ç½®
        self.task_start_delay = 30  # æ¯ä¸ªä»»åŠ¡å¯åŠ¨åç­‰å¾…ç§’æ•°
    
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        os.makedirs(log_dir, exist_ok=True)
        log_file = self._get_next_log_file()
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        logging.info(f"ğŸ“ Log file: {log_file}")
    
    def _get_next_log_file(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªæ—¥å¿—æ–‡ä»¶å"""
        base = os.path.join(log_dir, 'compete_gpu')
        if not os.path.exists(f"{base}.log"):
            return f"{base}.log"
        i = 1
        while os.path.exists(f"{base}({i}).log"):
            i += 1
        return f"{base}({i}).log"
    
    def _setup_tasks(self):
        """ä»å‘½ä»¤æ–‡ä»¶åˆå§‹åŒ–ä»»åŠ¡åˆ—è¡¨"""
        command_tasks = parse_command_file(commands_path)
        
        if not command_tasks:
            logging.warning(f"No tasks found in {commands_path}")
            return
        
        for commands, queue_id, memory in command_tasks:
            task = Task(
                commands=commands,
                queue_id=queue_id,
                estimated_memory_gb=memory,
                uni_id=generate_uni_id()
            )
            self.tasks.append(task)
            
            if queue_id not in self.queues:
                self.queues[queue_id] = []
            self.queues[queue_id].append(task)
        
        logging.info(f"ğŸ“‹ Total tasks: {len(self.tasks)}, Queues: {list(self.queues.keys())}")
        for qid, tasks in self.queues.items():
            logging.info(f"   Queue {qid}: {len(tasks)} tasks")
    
    def find_available_gpu(self, required_memory: int, exclude_gpus: set = None) -> Optional[int]:
        """æŸ¥æ‰¾å¯ç”¨çš„ GPU
        
        æ¡ä»¶ï¼š
        1. æœ‰è¶³å¤Ÿçš„æ˜¾å­˜
        2. éæé™æ¨¡å¼ä¸‹ï¼Œå½“å‰ç”¨æˆ·æ²¡æœ‰å…¶ä»– Python è¿›ç¨‹åœ¨è¯¥ GPU ä¸Š
        """
        exclude_gpus = exclude_gpus or set()
        
        for gpu_id in self.gpus:
            if gpu_id in exclude_gpus:
                continue
            
            # æ£€æŸ¥æ˜¾å­˜
            available = GPUMonitor.get_available_memory(gpu_id)
            if available < required_memory:
                logging.debug(f"GPU {gpu_id}: insufficient memory ({available:.1f}GB < {required_memory}GB)")
                continue
            
            # éæé™æ¨¡å¼ï¼šæ£€æŸ¥ç”¨æˆ·è¿›ç¨‹
            if not maximize_resource_utilization:
                user_procs = GPUMonitor.get_user_processes_on_gpu(gpu_id)
                if user_procs:
                    logging.debug(f"GPU {gpu_id}: user processes exist {user_procs}")
                    continue
            
            logging.info(f"âœ… GPU {gpu_id} available: {available:.1f}GB free")
            return gpu_id
        
        return None
    
    def get_busy_queues(self) -> set:
        """è·å–å½“å‰æ­£åœ¨è¿è¡Œä»»åŠ¡çš„é˜Ÿåˆ— ID é›†åˆ
        
        é€šè¿‡æ£€æŸ¥ JSON ä¸­ state=running ä¸”è¿›ç¨‹ç¡®å®å­˜åœ¨çš„è®°å½•
        """
        busy = set()
        running = self.process_json.get_running_processes()
        
        for uni_id, record in running.items():
            pid = record.get('pid', 0)
            if pid > 0 and psutil.pid_exists(pid):
                # æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡è·å–é˜Ÿåˆ— ID
                for task in self.tasks:
                    if task.uni_id == uni_id:
                        busy.add(task.queue_id)
                        break
        
        return busy
    
    def get_occupied_gpus(self) -> set:
        """è·å–å½“å‰è¢«å ç”¨çš„ GPU é›†åˆï¼ˆéæé™æ¨¡å¼ä¸‹ï¼‰"""
        if maximize_resource_utilization:
            return set()
        
        occupied = set()
        for gpu_id in self.gpus:
            user_procs = GPUMonitor.get_user_processes_on_gpu(gpu_id)
            if user_procs:
                occupied.add(gpu_id)
        return occupied
    
    def get_queue_head_task(self, queue_id: int) -> Optional[Task]:
        """è·å–é˜Ÿåˆ—çš„ç¬¬ä¸€ä¸ª pending ä»»åŠ¡"""
        for task in self.queues.get(queue_id, []):
            if task.status == "pending":
                return task
        return None
    
    def execute_task(self, task: Task, gpu_id: int) -> bool:
        """æ‰§è¡Œä»»åŠ¡ï¼ˆåŒæ­¥æ‰§è¡Œæ‰€æœ‰å‘½ä»¤ï¼‰
        
        Returns:
            True å¦‚æœä»»åŠ¡æˆåŠŸå¯åŠ¨ï¼ˆbash è„šæœ¬å·²å¯åŠ¨åå°è¿›ç¨‹ï¼‰
        """
        task.assigned_gpu = gpu_id
        task.status = "running"
        
        logging.info(f"ğŸš€ Starting task {task.uni_id} (Queue {task.queue_id}) on GPU {gpu_id}")
        
        for i, cmd_template in enumerate(task.commands):
            # æ›¿æ¢å˜é‡
            cmd = cmd_template.format(
                work_dir=work_dir,
                uni_id=task.uni_id
            )
            
            # åœ¨å‘½ä»¤å‰æ·»åŠ  CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿å­è¿›ç¨‹ç»§æ‰¿ï¼‰
            full_cmd = f"CUDA_VISIBLE_DEVICES={gpu_id} {cmd}"
            
            logging.info(f"   [{i+1}/{len(task.commands)}] [GPU {gpu_id}] {cmd[:80]}...")
            
            try:
                result = subprocess.run(
                    full_cmd, shell=True, capture_output=True, text=True, timeout=300
                )
                
                if result.returncode != 0:
                    logging.error(f"   Command failed: {result.stderr[:200]}")
                    task.status = "failed"
                    return False
                
                # æ‰“å°è¾“å‡ºï¼ˆç®€åŒ–ï¼‰
                if result.stdout.strip():
                    for line in result.stdout.strip().split('\n')[:5]:
                        logging.info(f"   > {line[:100]}")
                        
            except subprocess.TimeoutExpired:
                logging.error(f"   Command timeout")
                task.status = "failed"
                return False
            except Exception as e:
                logging.error(f"   Command error: {e}")
                task.status = "failed"
                return False
        
        task.status = "completed"
        logging.info(f"âœ… Task {task.uni_id} commands completed, waiting for background process...")
        return True
    
    def wait_for_process_start(self, task: Task, timeout: int = 60) -> bool:
        """ç­‰å¾…ä»»åŠ¡çš„ Python è¿›ç¨‹çœŸæ­£å¯åŠ¨
        
        é€šè¿‡æ£€æŸ¥ JSON ä¸­çš„ PID æ˜¯å¦æœ‰æ•ˆæ¥ç¡®è®¤
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            record = self.process_json.get_record(task.uni_id)
            if record:
                pid = record.get('pid', 0)
                state = record.get('state', '')
                
                if state == 'running' and pid > 0 and psutil.pid_exists(pid):
                    task.pid = pid
                    logging.info(f"âœ… Process confirmed: {task.uni_id} PID={pid}")
                    return True
                elif state in ('normal_exit', 'abnormal_exit'):
                    logging.warning(f"âš ï¸ Process already exited: {task.uni_id} state={state}")
                    return True  # è¿›ç¨‹å·²ç»“æŸï¼Œä¹Ÿç®—ç¡®è®¤
            
            time.sleep(2)
        
        logging.warning(f"â° Timeout waiting for process: {task.uni_id}")
        return False
    
    def print_status(self):
        """æ‰“å°å½“å‰çŠ¶æ€"""
        pending = sum(1 for t in self.tasks if t.status == "pending")
        running = sum(1 for t in self.tasks if t.status == "running")
        completed = sum(1 for t in self.tasks if t.status == "completed")
        failed = sum(1 for t in self.tasks if t.status == "failed")
        
        logging.info("=" * 60)
        logging.info(f"ğŸ“Š Tasks: Pending={pending}, Running={running}, Completed={completed}, Failed={failed}")
        
        busy_queues = self.get_busy_queues()
        for qid in sorted(self.queues.keys()):
            status = "ğŸ”´ BUSY" if qid in busy_queues else "ğŸŸ¢ IDLE"
            q_pending = sum(1 for t in self.queues[qid] if t.status == "pending")
            q_completed = sum(1 for t in self.queues[qid] if t.status == "completed")
            logging.info(f"   Queue {qid}: {status}, Pending={q_pending}, Completed={q_completed}")
        
        logging.info("=" * 60)
    
    def check_and_handle_finished_tasks(self):
        """æ£€æŸ¥å·²å®Œæˆ/å¼‚å¸¸çš„ä»»åŠ¡å¹¶å¤„ç†é‡è¯•
        
        é€šè¿‡ JSON æ–‡ä»¶æ£€æµ‹è¿›ç¨‹çŠ¶æ€å˜åŒ–
        """
        for task in self.tasks:
            if task.status != "running" and task.status != "completed":
                continue
            
            record = self.process_json.get_record(task.uni_id)
            if not record:
                continue
            
            state = record.get('state', '')
            
            if state == 'normal_exit':
                # æ­£å¸¸é€€å‡º
                if task.status != "completed":
                    task.status = "completed"
                    logging.info(f"âœ… Task {task.uni_id} (Queue {task.queue_id}) completed successfully")
            
            elif state == 'abnormal_exit':
                # å¼‚å¸¸é€€å‡ºï¼Œéœ€è¦é‡è¯•
                if task.status == "running":
                    error_type = record.get('error_type', 'unknown')
                    handle_task_retry(task, error_type, retry_config)
    
    def run(self):
        """ä¸»è°ƒåº¦å¾ªç¯
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. æ£€æŸ¥å·²å®Œæˆ/å¼‚å¸¸çš„ä»»åŠ¡ï¼Œå¤„ç†é‡è¯•
        2. è·å–æ‰€æœ‰ç©ºé—²é˜Ÿåˆ—çš„å¤´éƒ¨ä»»åŠ¡
        3. éšæœºæ‰“ä¹±é¡ºåºï¼ˆå…¬å¹³è°ƒåº¦ï¼‰
        4. é€ä¸ªåˆ†é…ä»»åŠ¡åˆ°å¯ç”¨ GPU
        5. æ¯åˆ†é…ä¸€ä¸ªä»»åŠ¡åç­‰å¾…ç¡®è®¤ï¼Œå†åˆ†é…ä¸‹ä¸€ä¸ª
        """
        logging.info("ğŸ Starting GPU competition scheduler")
        self.print_status()
        
        try:
            while self.running:
                # æ£€æŸ¥å·²å®Œæˆ/å¼‚å¸¸çš„ä»»åŠ¡
                self.check_and_handle_finished_tasks()
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆ
                incomplete_tasks = [t for t in self.tasks if t.status != "completed"]
                if not incomplete_tasks:
                    logging.info("ğŸ‰ All tasks completed!")
                    break
                
                # è·å–å½“å‰å¿™ç¢Œçš„é˜Ÿåˆ—
                busy_queues = self.get_busy_queues()
                
                # è·å–ç©ºé—²é˜Ÿåˆ—çš„å¤´éƒ¨ä»»åŠ¡ï¼ˆè€ƒè™‘é€€é¿ï¼‰
                candidate_tasks = []
                for qid in self.queues.keys():
                    if qid not in busy_queues:
                        head_task = self.get_queue_head_task(qid)
                        if head_task and is_task_ready(head_task):
                            candidate_tasks.append(head_task)
                
                if not candidate_tasks:
                    # æ²¡æœ‰å¯è°ƒåº¦çš„ä»»åŠ¡ï¼Œç­‰å¾…
                    time.sleep(check_time)
                    continue
                
                # éšæœºæ‰“ä¹±é¡ºåºï¼ˆå…¬å¹³è°ƒåº¦ï¼‰
                random.shuffle(candidate_tasks)
                
                # è·å–å½“å‰è¢«å ç”¨çš„ GPU
                occupied_gpus = self.get_occupied_gpus()
                
                # é€ä¸ªåˆ†é…ä»»åŠ¡
                tasks_started = 0
                for task in candidate_tasks:
                    # é‡æ–°è·å–å ç”¨çš„ GPUï¼ˆå®æ—¶æ£€æµ‹ï¼‰
                    occupied_gpus = self.get_occupied_gpus()
                    
                    # æŸ¥æ‰¾å¯ç”¨ GPU
                    gpu_id = self.find_available_gpu(task.estimated_memory_gb, occupied_gpus)
                    
                    if gpu_id is None:
                        logging.info(f"â³ No GPU available for task {task.uni_id} (need {task.estimated_memory_gb}GB)")
                        continue
                    
                    # æ‰§è¡Œä»»åŠ¡
                    success = self.execute_task(task, gpu_id)
                    
                    if success:
                        tasks_started += 1
                        
                        # ç­‰å¾…è¿›ç¨‹ç¡®è®¤å¯åŠ¨
                        self.wait_for_process_start(task, timeout=60)
                        
                        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†åˆ†é…ä¸‹ä¸€ä¸ªä»»åŠ¡
                        logging.info(f"â³ Waiting {self.task_start_delay}s before next task...")
                        time.sleep(self.task_start_delay)
                
                if tasks_started > 0:
                    logging.info(f"ğŸ“ˆ Started {tasks_started} task(s) this round")
                
                # æ‰“å°çŠ¶æ€
                self.print_status()
                
                # ç­‰å¾…ä¸‹ä¸€è½®è°ƒåº¦
                time.sleep(check_time)
                
        except KeyboardInterrupt:
            logging.info("ğŸ›‘ Interrupted by user")
            self.running = False
        
        logging.info("ğŸ Scheduler stopped")


# =============================================================================
# ä¸»å…¥å£
# =============================================================================

if __name__ == "__main__":
    competitor = GPUCompetitor()
    competitor.run()
