#!/usr/bin/env python3
"""
Multi-GPU Competition Script - å¤šGPUç«äº‰è°ƒåº¦å™¨
Manages multi-GPU resource allocation based on available memory

æ ¸å¿ƒç‰¹æ€§ï¼š
- é˜Ÿå†…ä¸²è¡Œï¼šåŒä¸€é˜Ÿåˆ—çš„ä»»åŠ¡ä¸¥æ ¼æŒ‰é¡ºåºæ‰§è¡Œ
- é˜Ÿé—´å¹¶è¡Œï¼šä¸åŒé˜Ÿåˆ—çš„ä»»åŠ¡å¯ä»¥åŒæ—¶åœ¨ä¸åŒ GPU ä¸Šæ‰§è¡Œ
- æ”¯æŒä»»åŠ¡ä½¿ç”¨å¤šå¼ GPU
- é‡è¯•æœºåˆ¶ï¼šä»»åŠ¡å¤±è´¥åæ ¹æ®é…ç½®è¿›è¡Œé‡è¯•å’Œé€€é¿

æ¨¡å—ç»“æ„ï¼š
- utils/gpu_monitor.py: GPU çŠ¶æ€ç›‘æ§
- utils/retry.py: é‡è¯•æœºåˆ¶
- utils/gpus_command_file.py: å¤šGPUå‘½ä»¤æ–‡ä»¶è§£æ
- command_gpus.txt: å¤šGPUä»»åŠ¡é…ç½®æ–‡ä»¶
"""

import os
import sys
import time
import subprocess
import logging
import argparse
from typing import Dict, List, Optional, Set
from dataclasses import dataclass, field
import psutil
import threading
import concurrent.futures

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
# è„šæœ¬ç›®å½•ï¼ˆappç›®å½•çš„çˆ¶ç›®å½•ï¼‰
SCRIPT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, SCRIPT_DIR)

# å¯¼å…¥å·¥å…·æ¨¡å—
from app.utils.gpu_monitor import GPUMonitor
from app.utils.retry import RetryConfig
from app.utils.gpus_command_file import parse_command_file
from app.utils.process_yaml import ProcessYAML, load_config_with_args, parse_command_file_path, resolve_work_dir
from app.utils.gpu_select import GPUSelector, select_gpus
from app.utils.update_state import StatusWriter, get_status_writer

# è¿è¡Œæ–¹å¼:
# nohup python main_gpus.py > /dev/null 2>&1 &
# nohup python main_gpus.py --command-file /path/to/custom_command_gpus.txt > /dev/null 2>&1 &

# =============================================================================
# å‘½ä»¤è¡Œå‚æ•°è§£æ
# =============================================================================

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Multi-GPU Competition Script - å¤šGPUç«äº‰è°ƒåº¦å™¨')
    parser.add_argument('--command-file', 
                       type=str,
                       help='å‘½ä»¤é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: command/command_gpus.txtï¼‰',
                       default=None)
    parser.add_argument('--config-file',
                       type=str,
                       help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config.yamlï¼‰',
                       default=None)
    parser.add_argument('--config-index',
                       type=int,
                       help='é…ç½®ç´¢å¼•ï¼ˆç”¨äºçŠ¶æ€è·Ÿè¸ªï¼‰',
                       default=0)
    return parser.parse_args()

# è§£æå‘½ä»¤è¡Œå‚æ•°
args = parse_arguments()

# =============================================================================
# é…ç½®é¡¹
# =============================================================================

# æ—¥å¿—ç›®å½•é…ç½®
log_dir = os.path.join(SCRIPT_DIR, 'logs')  # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ logs ç›®å½•

# å‘½ä»¤æ–‡ä»¶è·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
commands_path = parse_command_file_path(args, SCRIPT_DIR, 'command/command_gpus.txt')

# åŠ è½½é…ç½®
config_processor, config = load_config_with_args(args, SCRIPT_DIR, 'config/gpu_manage.yaml')

# å·¥ä½œç›®å½•é…ç½®ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
work_dir = resolve_work_dir(config, SCRIPT_DIR)

# è°ƒåº¦é…ç½®
check_time = config.get('check_time', 5)  # è°ƒåº¦é—´éš”ï¼ˆç§’ï¼‰
maximize_resource_utilization = config.get('maximize_resource_utilization', False)  # æé™åˆ©ç”¨èµ„æºæ¨¡å¼
memory_save_mode = config.get('memory_save_mode', True)  # GPUé€‰æ‹©æ¨¡å¼ï¼šTrue=èŠ‚çœæ˜¾å­˜ï¼ŒFalse=é˜²æ­¢æº¢å‡º

# GPU é…ç½®
compete_gpus = config.get('compete_gpus', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  # æ‰‹åŠ¨æŒ‡å®šçš„ GPU åˆ—è¡¨
use_all_gpus = config.get('use_all_gpus', True)  # æ˜¯å¦è‡ªåŠ¨æ¢æµ‹æ‰€æœ‰ GPU
gpu_left = config.get('gpu_left', 0)  # å‰©ä½™å‡ å¼ å¡ç»™å…¶ä»–ç”¨æˆ·
min_gpu = config.get('min_gpu', 3)  # ç”¨æˆ·è‡³å°‘ç”¨å‡ å¼ å¡
max_gpu = config.get('max_gpu', 8)  # ç”¨æˆ·æœ€å¤šç”¨å‡ å¼ å¡

# é‡è¯•é…ç½®
retry_config_dict = config.get('retry_config', {})
retry_config = RetryConfig(
    max_retry_before_backoff=retry_config_dict.get('max_retry_before_backoff', 3),  # æ¯ 3 æ¬¡é‡è¯•åè¿›å…¥é€€é¿
    backoff_duration=retry_config_dict.get('backoff_duration', 600)         # é€€é¿æ—¶é—´ 10 åˆ†é’Ÿ
)



# =============================================================================
# ä»»åŠ¡æ•°æ®ç»“æ„ï¼ˆå¤šGPUç‰ˆæœ¬ï¼‰
# =============================================================================

@dataclass
class Task:
    """å¤šGPUä»»åŠ¡æ•°æ®ç»“æ„"""
    commands: List[str]          # å‘½ä»¤åˆ—è¡¨ï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰
    queue_id: int                # é˜Ÿåˆ— ID
    gpu_count: int               # GPU æ•°é‡éœ€æ±‚
    estimated_memory_gb: int     # æ¯å¼ GPUé¢„ä¼°æ˜¾å­˜ (GB)
    status: str = "pending"      # pending / running / completed / failed
    assigned_gpus: List[int] = field(default_factory=list)  # åˆ†é…çš„ GPU ID åˆ—è¡¨
    retry_count: int = 0         # é‡è¯•æ¬¡æ•°
    backoff_until: float = 0     # é€€é¿ç»“æŸæ—¶é—´æˆ³
    error_type: str = ""         # é”™è¯¯ç±»å‹


class MultiGPUCompetitor:
    """å¤šGPUç«äº‰è°ƒåº¦å™¨ - æ ¸å¿ƒç±»
    
    æ ¸å¿ƒé€»è¾‘ï¼šé˜Ÿåˆ—å†…ä¸²è¡Œï¼Œé˜Ÿåˆ—é—´å¹¶è¡Œ
    - åŒä¸€é˜Ÿåˆ—çš„ä»»åŠ¡ä¸¥æ ¼æŒ‰é¡ºåºæ‰§è¡Œ
    - ä¸åŒé˜Ÿåˆ—çš„ä»»åŠ¡å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼ˆåœ¨ä¸åŒ GPU ä¸Šï¼‰
    - æ”¯æŒä»»åŠ¡ä½¿ç”¨å¤šå¼ GPU
    - ä½¿ç”¨çº¿ç¨‹æ± å®ç°é˜Ÿé—´å¹¶è¡Œ
    """
    
    def __init__(self):
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ– GPU åˆ—è¡¨ï¼ˆåŠ¨æ€é¢„ç•™ï¼šå¯ä»¥åœ¨æ‰€æœ‰å¡ä¸Šè¿è¡Œï¼‰
        if use_all_gpus:
            all_gpus = GPUMonitor.detect_gpus()
        else:
            all_gpus = compete_gpus
        
        # åŠ¨æ€é¢„ç•™æ¨¡å¼ï¼šæ‰€æœ‰GPUéƒ½å¯ä»¥ä½¿ç”¨ï¼Œè¿è¡Œæ—¶åŠ¨æ€è®¡ç®—å¯ç”¨é…é¢
        self.gpus = all_gpus
        self.total_gpus = len(all_gpus)
        
        # ä¿å­˜é…ç½®åˆ°å®ä¾‹å˜é‡
        self.gpu_left = gpu_left
        self.min_gpu = min_gpu
        self.max_gpu = max_gpu
        
        logging.info(f"ğŸ–¥ï¸ Total GPUs available: {self.gpus}")
        logging.info(f"ğŸ–¥ï¸ Dynamic reservation config: gpu_left={gpu_left}, min_gpu={min_gpu}, max_gpu={max_gpu}")
        
        # çº¿ç¨‹åŒæ­¥
        self.gpu_lock = threading.Lock()  # GPU åˆ†é…é”
        self.queue_locks: Dict[int, threading.Lock] = {}  # æ¯ä¸ªé˜Ÿåˆ—ä¸€ä¸ªé”
        
        # GPU å ç”¨çŠ¶æ€ï¼ˆè°ƒåº¦å™¨å†…éƒ¨ç»´æŠ¤ï¼Œä¸ä¾èµ–nvidia-smiæ£€æµ‹å»¶è¿Ÿï¼‰
        self.occupied_gpus: Dict[int, int] = {}  # gpu_id -> queue_idï¼ˆæ­£åœ¨ä½¿ç”¨è¯¥GPUçš„é˜Ÿåˆ—ï¼‰
        
        # é˜Ÿåˆ—æ‰§è¡ŒçŠ¶æ€
        self.queue_futures: Dict[int, concurrent.futures.Future] = {}  # é˜Ÿåˆ— -> Future
        
        # åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
        self.tasks: List[Task] = []
        self.queues: Dict[int, List[Task]] = {}  # queue_id -> [tasks]
        self._setup_tasks()
        
        # è¿è¡ŒçŠ¶æ€
        self.running = True
        
        # çŠ¶æ€å†™å…¥å™¨
        self.status_writer: StatusWriter = None
    
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        log_file = self._get_next_log_file()
        
        # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
        root_logger = logging.getLogger()
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
        
        # é…ç½®æ–°çš„å¤„ç†å™¨
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ],
            force=True
        )
        logging.info(f"ğŸ“ Log file: {log_file}")
        logging.info(f"ğŸ“„ Command file: {commands_path}")
    
    def _get_next_log_file(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªæ—¥å¿—æ–‡ä»¶å"""
        base = os.path.join(log_dir, 'compete_gpus')  # å¤šGPUä¸“ç”¨æ—¥å¿—
        if not os.path.exists(f"{base}.log"):
            return f"{base}.log"
        i = 1
        while os.path.exists(f"{base}({i}).log"):
            i += 1
        return f"{base}({i}).log"
    
    def _setup_tasks(self):
        """ä»å‘½ä»¤æ–‡ä»¶åˆå§‹åŒ–ä»»åŠ¡åˆ—è¡¨ï¼ˆå¤šGPUç‰ˆæœ¬ï¼‰"""
        command_tasks = parse_command_file(commands_path)
        
        if not command_tasks:
            logging.warning(f"No tasks found in {commands_path}")
            return
        
        for commands, queue_id, gpu_count, memory in command_tasks:
            task = Task(
                commands=commands,
                queue_id=queue_id,
                gpu_count=gpu_count,
                estimated_memory_gb=memory
            )
            self.tasks.append(task)
            
            if queue_id not in self.queues:
                self.queues[queue_id] = []
            self.queues[queue_id].append(task)
        
        logging.info(f"ğŸ“‹ Total tasks: {len(self.tasks)}, Queues: {list(self.queues.keys())}")
        for qid, tasks in self.queues.items():
            gpu_counts = [t.gpu_count for t in tasks]
            logging.info(f"   Queue {qid}: {len(tasks)} tasks, GPU needs: {gpu_counts}")
            # ä¸ºæ¯ä¸ªé˜Ÿåˆ—åˆ›å»ºé”
            self.queue_locks[qid] = threading.Lock()
    
    def _get_current_user_gpu_count(self) -> int:
        """è·å–å½“å‰ç”¨æˆ·æ­£åœ¨ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆè°ƒåº¦å™¨å†…éƒ¨å ç”¨ + å¤–éƒ¨è¿›ç¨‹å ç”¨ï¼‰"""
        user_gpu_count = 0
        for gpu_id in self.gpus:
            # æ£€æŸ¥è°ƒåº¦å™¨å†…éƒ¨å ç”¨
            if gpu_id in self.occupied_gpus:
                user_gpu_count += 1
                continue
            # æ£€æŸ¥å¤–éƒ¨ç”¨æˆ·è¿›ç¨‹
            user_procs = GPUMonitor.get_user_processes_on_gpu(gpu_id)
            if user_procs:
                user_gpu_count += 1
        return user_gpu_count
    
    def _get_max_allowed_gpus(self) -> int:
        """åŠ¨æ€è®¡ç®—å½“å‰å…è®¸ä½¿ç”¨çš„æœ€å¤§GPUæ•°é‡
        
        å…¬å¼ï¼šmin(max_gpu, max(min_gpu, available_gpus - gpu_left))
        å…¶ä¸­ available_gpus æ˜¯å½“å‰æ˜¾å­˜å……è¶³çš„GPUæ•°é‡ï¼ˆä¸è€ƒè™‘ç”¨æˆ·å ç”¨ï¼‰
        """
        # ç»Ÿè®¡æ˜¾å­˜å……è¶³çš„GPUæ•°é‡ï¼ˆavailable_gpusï¼‰
        available_gpus = 0
        for gpu_id in self.gpus:
            available_mem = GPUMonitor.get_available_memory(gpu_id)
            if available_mem >= 1:  # è‡³å°‘1GBå¯ç”¨æ˜¾å­˜æ‰ç®—å¯ç”¨
                available_gpus += 1
        
        # è®¡ç®—å…è®¸ä½¿ç”¨çš„æœ€å¤§GPUæ•°é‡
        max_allowed = min(self.max_gpu, max(self.min_gpu, available_gpus - self.gpu_left))
        return max(0, max_allowed)
    
    def _can_acquire_more_gpus(self, count: int = 1) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å†è·å–æ›´å¤šGPU
        
        Args:
            count: éœ€è¦è·å–çš„GPUæ•°é‡
        """
        current_used = self._get_current_user_gpu_count()
        max_allowed = self._get_max_allowed_gpus()
        return current_used + count <= max_allowed

    def find_available_gpus(self, gpu_count: int, required_memory: int, queue_id: int = -1) -> Optional[List[int]]:
        """æŸ¥æ‰¾å¤šä¸ªå¯ç”¨çš„ GPU
        
        æ¡ä»¶ï¼š
        1. åŠ¨æ€é¢„ç•™æ£€æŸ¥ï¼šå½“å‰ç”¨æˆ·ä½¿ç”¨çš„GPUæ•°é‡æœªè¶…è¿‡å…è®¸çš„æœ€å¤§å€¼
        2. æœ‰è¶³å¤Ÿçš„æ˜¾å­˜
        3. éæé™æ¨¡å¼ä¸‹ï¼š
           a. è°ƒåº¦å™¨å†…éƒ¨æ²¡æœ‰å…¶ä»–ä»»åŠ¡æ­£åœ¨ä½¿ç”¨è¯¥GPU
           b. å½“å‰ç”¨æˆ·æ²¡æœ‰å…¶ä»– Python è¿›ç¨‹åœ¨è¯¥ GPU ä¸Šï¼ˆå¤–éƒ¨è¿›ç¨‹ï¼‰
        4. å¤šä¸ªå¯ç”¨GPUæ—¶ï¼Œä½¿ç”¨æ™ºèƒ½é€‰æ‹©ç­–ç•¥
        
        Args:
            gpu_count: éœ€è¦çš„ GPU æ•°é‡
            required_memory: æ¯å¼  GPU éœ€è¦çš„æ˜¾å­˜ (GB)
            queue_id: è¯·æ±‚GPUçš„é˜Ÿåˆ—IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            å¯ç”¨çš„ GPU ID åˆ—è¡¨ï¼Œå¦‚æœä¸è¶³åˆ™è¿”å› None
        """
        # åŠ¨æ€é¢„ç•™æ£€æŸ¥ï¼šæ˜¯å¦è¿˜èƒ½è·å–æ›´å¤šGPU
        if not self._can_acquire_more_gpus(gpu_count):
            current_used = self._get_current_user_gpu_count()
            max_allowed = self._get_max_allowed_gpus()
            logging.debug(f"Dynamic reservation limit reached: using {current_used}/{max_allowed} GPUs, need {gpu_count} more")
            return None
        
        # ç¬¬ä¸€æ­¥ï¼šç­›é€‰å‡ºæ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„GPU
        candidate_gpus = []
        for gpu_id in self.gpus:
            # éæé™æ¨¡å¼ï¼šæ£€æŸ¥è°ƒåº¦å™¨å†…éƒ¨å ç”¨
            if not maximize_resource_utilization:
                if gpu_id in self.occupied_gpus:
                    occupying_queue = self.occupied_gpus[gpu_id]
                    logging.debug(f"GPU {gpu_id}: occupied by queue {occupying_queue} (internal)")
                    continue
            
            # æ£€æŸ¥æ˜¾å­˜
            available = GPUMonitor.get_available_memory(gpu_id)
            if available < required_memory:
                logging.debug(f"GPU {gpu_id}: insufficient memory ({available:.1f}GB < {required_memory}GB)")
                continue
            
            # éæé™æ¨¡å¼ï¼šæ£€æŸ¥å¤–éƒ¨ç”¨æˆ·è¿›ç¨‹
            if not maximize_resource_utilization:
                user_procs = GPUMonitor.get_user_processes_on_gpu(gpu_id)
                if user_procs:
                    logging.debug(f"GPU {gpu_id}: external user processes exist {user_procs}")
                    continue
            
            candidate_gpus.append(gpu_id)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å€™é€‰GPU
        if len(candidate_gpus) < gpu_count:
            logging.debug(f"Only found {len(candidate_gpus)} GPUs, need {gpu_count}")
            return None
        
        # ç¬¬äºŒæ­¥ï¼šå¦‚æœå€™é€‰GPUæ•°é‡åˆšå¥½ç­‰äºéœ€æ±‚ï¼Œç›´æ¥è¿”å›
        if len(candidate_gpus) == gpu_count:
            logging.info(f"âœ… Found exactly {gpu_count} GPUs: {candidate_gpus}")
            return candidate_gpus
        
        # ç¬¬ä¸‰æ­¥ï¼šå¤šä¸ªå€™é€‰GPUæ—¶ï¼Œä½¿ç”¨æ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼ˆé«˜é¢‘é‡‡æ ·ï¼‰
        logging.info(f"ğŸ” å‘ç° {len(candidate_gpus)} ä¸ªå€™é€‰GPU: {candidate_gpus}ï¼Œéœ€è¦ {gpu_count} ä¸ªï¼Œå¯åŠ¨æ™ºèƒ½é€‰æ‹©...")
        
        best_gpus = select_gpus(
            gpu_ids=candidate_gpus,
            count=gpu_count,
            memory_save_mode=memory_save_mode,
            required_memory=required_memory,
            use_sampling=True  # ä½¿ç”¨3ç§’30æ¬¡é‡‡æ ·
        )
        
        if len(best_gpus) >= gpu_count:
            return best_gpus[:gpu_count]
        
        # å¦‚æœæ™ºèƒ½é€‰æ‹©è¿”å›çš„GPUä¸è¶³ï¼Œå›é€€åˆ°å‰Nä¸ªå€™é€‰GPU
        logging.warning(f"æ™ºèƒ½é€‰æ‹©è¿”å› {len(best_gpus)} ä¸ªGPUï¼Œä¸è¶³ {gpu_count} ä¸ªï¼Œå›é€€åˆ°å€™é€‰åˆ—è¡¨")
        return candidate_gpus[:gpu_count]
    
    def _acquire_gpus(self, gpu_ids: List[int], queue_id: int):
        """æ ‡è®°å¤šä¸ªGPUä¸ºå·²å ç”¨"""
        with self.gpu_lock:
            for gpu_id in gpu_ids:
                self.occupied_gpus[gpu_id] = queue_id
            logging.info(f"ğŸ”’ GPUs {gpu_ids} acquired by queue {queue_id}")
    
    def _release_gpus(self, gpu_ids: List[int], queue_id: int):
        """é‡Šæ”¾å¤šä¸ªGPUå ç”¨"""
        with self.gpu_lock:
            for gpu_id in gpu_ids:
                if gpu_id in self.occupied_gpus and self.occupied_gpus[gpu_id] == queue_id:
                    del self.occupied_gpus[gpu_id]
            logging.info(f"ğŸ”“ GPUs {gpu_ids} released by queue {queue_id}")
    
    def get_busy_queues(self) -> set:
        """è·å–å½“å‰æ­£åœ¨è¿è¡Œä»»åŠ¡çš„é˜Ÿåˆ— ID é›†åˆ
        
        ç›´æ¥æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        """
        busy = set()
        for task in self.tasks:
            if task.status == "running":
                busy.add(task.queue_id)
        return busy
    
    def get_occupied_gpus(self) -> set:
        """è·å–å½“å‰è¢«å ç”¨çš„ GPU é›†åˆï¼ˆéæé™æ¨¡å¼ä¸‹ï¼‰"""
        if maximize_resource_utilization:
            return set()
        
        occupied = set()
        for gpu_id in self.gpus:
            user_procs = GPUMonitor.get_user_processes_on_gpu(gpu_id)
            if user_procs:
                occupied.add(gpu_id)
        return occupied
    
    def get_queue_head_task(self, queue_id: int) -> Optional[Task]:
        """è·å–é˜Ÿåˆ—çš„ç¬¬ä¸€ä¸ª pending ä»»åŠ¡"""
        for task in self.queues.get(queue_id, []):
            if task.status == "pending":
                return task
        return None
    
    def execute_task(self, task: Task, gpu_ids: List[int]) -> bool:
        """æ‰§è¡Œå¤šGPUä»»åŠ¡ï¼ˆåŒæ­¥æ‰§è¡Œæ‰€æœ‰å‘½ä»¤ï¼‰
        
        Args:
            task: ä»»åŠ¡å¯¹è±¡
            gpu_ids: åˆ†é…çš„ GPU ID åˆ—è¡¨
            
        Returns:
            True å¦‚æœæ‰€æœ‰å‘½ä»¤æˆåŠŸæ‰§è¡Œ
            False å¦‚æœä»»ä½•å‘½ä»¤å¤±è´¥ï¼ˆä¼šè§¦å‘é‡è¯•æœºåˆ¶ï¼‰
        """
        task.assigned_gpus = gpu_ids
        task.status = "running"
        
        # æ„å»º CUDA_VISIBLE_DEVICES å­—ç¬¦ä¸²
        cuda_devices = ','.join(map(str, gpu_ids))
        
        logging.info(f"ğŸš€ Starting task (Queue {task.queue_id}, retry={task.retry_count}) on GPUs {gpu_ids}")
        
        for i, cmd_template in enumerate(task.commands):
            # æ›¿æ¢å˜é‡
            cmd = cmd_template.format(work_dir=work_dir)
            
            # åœ¨å‘½ä»¤å‰æ·»åŠ  CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡
            # ä½¿ç”¨ç»å¯¹è·¯å¾„åˆå§‹åŒ– condaï¼Œé¿å… HOME ç¯å¢ƒå˜é‡é—®é¢˜
            home_dir = os.path.expanduser('~')
            conda_sh = f"{home_dir}/miniconda3/etc/profile.d/conda.sh"
            full_cmd = f"source {conda_sh} && export CUDA_VISIBLE_DEVICES={cuda_devices} && {cmd}"
            
            logging.info(f"   [{i+1}/{len(task.commands)}] [GPUs {cuda_devices}] {cmd[:80]}...")
            
            try:
                # ç¡®ä¿ HOME ç¯å¢ƒå˜é‡è¢«æ­£ç¡®è®¾ç½®
                env = os.environ.copy()
                env['HOME'] = home_dir
                
                result = subprocess.run(
                    full_cmd,
                    shell=True,
                    executable='/bin/bash',
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    timeout=7200,  # 2å°æ—¶è¶…æ—¶
                    env=env
                )
                
                if result.returncode != 0:
                    error_msg = result.stderr[:500] if result.stderr else "Unknown error"
                    logging.error(f"   âŒ Command failed (exit code {result.returncode}): {error_msg}")
                    # è§¦å‘é‡è¯•æœºåˆ¶
                    self._handle_task_failure(task, f"exit_code_{result.returncode}")
                    return False
                
                # æ‰“å°è¾“å‡ºï¼ˆç®€åŒ–ï¼‰
                if result.stdout.strip():
                    for line in result.stdout.strip().split('\n')[:5]:
                        logging.info(f"   > {line[:100]}")
                        
            except subprocess.TimeoutExpired:
                logging.error(f"   âŒ Command timeout (2h)")
                self._handle_task_failure(task, "timeout")
                return False
            except Exception as e:
                logging.error(f"   âŒ Command error: {e}")
                self._handle_task_failure(task, str(type(e).__name__))
                return False
        
        task.status = "completed"
        logging.info(f"âœ… Task (Queue {task.queue_id}) completed successfully")
        return True
    
    def _handle_task_failure(self, task: Task, error_type: str):
        """å¤„ç†ä»»åŠ¡å¤±è´¥ï¼Œåº”ç”¨é‡è¯•æœºåˆ¶"""
        task.retry_count += 1
        task.error_type = error_type
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€é¿
        if task.retry_count % retry_config.max_retry_before_backoff == 0:
            task.backoff_until = time.time() + retry_config.backoff_duration
            task.status = "pending"
            logging.warning(
                f"ğŸ”„ Task (Queue {task.queue_id}) failed (retry #{task.retry_count}, error={error_type}), "
                f"entering backoff for {retry_config.backoff_duration // 60} minutes"
            )
        else:
            task.status = "pending"
            logging.warning(
                f"ğŸ”„ Task (Queue {task.queue_id}) failed (retry #{task.retry_count}, error={error_type}), "
                f"will retry soon"
            )
    
    def print_status(self):
        """æ‰“å°å½“å‰çŠ¶æ€"""
        pending = sum(1 for t in self.tasks if t.status == "pending")
        running = sum(1 for t in self.tasks if t.status == "running")
        completed = sum(1 for t in self.tasks if t.status == "completed")
        failed = sum(1 for t in self.tasks if t.status == "failed")
        
        logging.info("=" * 60)
        logging.info(f"ğŸ“Š Tasks: Pending={pending}, Running={running}, Completed={completed}, Failed={failed}")
        
        busy_queues = self.get_busy_queues()
        for qid in sorted(self.queues.keys()):
            status = "ğŸ”´ BUSY" if qid in busy_queues else "ğŸŸ¢ IDLE"
            q_pending = sum(1 for t in self.queues[qid] if t.status == "pending")
            q_completed = sum(1 for t in self.queues[qid] if t.status == "completed")
            logging.info(f"   Queue {qid}: {status}, Pending={q_pending}, Completed={q_completed}")
        
        logging.info("=" * 60)
    
    def _run_queue(self, queue_id: int):
        """è¿è¡Œå•ä¸ªé˜Ÿåˆ—çš„æ‰€æœ‰ä»»åŠ¡ï¼ˆé˜Ÿå†…ä¸²è¡Œï¼‰
        
        Args:
            queue_id: é˜Ÿåˆ— ID
        """
        tasks = self.queues.get(queue_id, [])
        logging.info(f"ğŸš€ Queue {queue_id}: Starting with {len(tasks)} tasks")
        
        for task_idx, task in enumerate(tasks):
            if not self.running:
                logging.info(f"ğŸ›‘ Queue {queue_id}: Scheduler stopped")
                break
            
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ
            if task.status == "completed":
                logging.info(f"â­ï¸ Queue {queue_id}: Task {task_idx+1}/{len(tasks)} already completed, skipping")
                continue
            
            # æ‰§è¡Œä»»åŠ¡ï¼ˆå¸¦é‡è¯•ï¼‰
            success = self._execute_task_with_retry(task, queue_id, task_idx, len(tasks))
            
            if not success:
                logging.error(f"âŒ Queue {queue_id}: Task {task_idx+1}/{len(tasks)} failed after all retries, stopping queue")
                break
        
        # é˜Ÿåˆ—å®Œæˆ
        completed = sum(1 for t in tasks if t.status == "completed")
        logging.info(f"ğŸ Queue {queue_id}: Finished. Completed {completed}/{len(tasks)} tasks")
        
        # æ›´æ–°çŠ¶æ€ï¼šé˜Ÿåˆ—å®Œæˆ
        if self.status_writer:
            if completed == len(tasks):
                self.status_writer.on_queue_complete(queue_id)
            else:
                self.status_writer.on_queue_fail(queue_id, f"Completed {completed}/{len(tasks)} tasks")
    
    def _execute_task_with_retry(self, task: Task, queue_id: int, task_idx: int, total_tasks: int) -> bool:
        """æ‰§è¡Œä»»åŠ¡ï¼Œå¸¦é‡è¯•æœºåˆ¶
        
        Returns:
            True å¦‚æœä»»åŠ¡æœ€ç»ˆæˆåŠŸ
            False å¦‚æœä»»åŠ¡å¤±è´¥ä¸”æ— æ³•é‡è¯•
        """
        max_total_retries = 100  # æœ€å¤§æ€»é‡è¯•æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        
        while task.retry_count < max_total_retries:
            if not self.running:
                return False
            
            # æ£€æŸ¥é€€é¿
            if task.backoff_until > 0 and time.time() < task.backoff_until:
                wait_time = task.backoff_until - time.time()
                logging.info(f"â³ Queue {queue_id}: Task {task_idx+1}/{total_tasks} in backoff, waiting {wait_time:.0f}s")
                time.sleep(min(wait_time, 60))  # æ¯æ¬¡æœ€å¤šç­‰60ç§’ï¼Œç„¶åé‡æ–°æ£€æŸ¥
                continue
            
            # ç­‰å¾…å¹¶è·å–å¯ç”¨ GPUï¼ˆä¼šç«‹å³æ ‡è®°ä¸ºå ç”¨ï¼‰
            gpu_ids = self._wait_for_gpus(task.gpu_count, task.estimated_memory_gb, queue_id)
            if gpu_ids is None:
                logging.error(f"âŒ Queue {queue_id}: Cannot find available GPUs, stopping")
                return False
            
            # æ‰§è¡Œä»»åŠ¡
            logging.info(f"ğŸ¯ Queue {queue_id}: Executing task {task_idx+1}/{total_tasks} on GPUs {gpu_ids} (retry={task.retry_count})")
            
            # æ›´æ–°çŠ¶æ€ï¼šä»»åŠ¡å¼€å§‹
            if self.status_writer:
                cmd_preview = task.commands[0][:50] if task.commands else ""
                self.status_writer.on_task_start(queue_id, task_idx, total_tasks, gpu_ids[0], cmd_preview)
                # æ›´æ–°è¿›ç¨‹çº§çŠ¶æ€
                from datetime import datetime
                self.status_writer.update_process_status(
                    queue_id, task_idx,
                    status="running",
                    current_gpu=gpu_ids[0],
                    gpus=gpu_ids,
                    retry_count=task.retry_count,
                    started_at=datetime.now().isoformat()
                )
            
            try:
                success = self.execute_task(task, gpu_ids)
            finally:
                # ä»»åŠ¡å®Œæˆåé‡Šæ”¾GPU
                self._release_gpus(gpu_ids, queue_id)
            
            if success:
                # æ›´æ–°çŠ¶æ€ï¼šä»»åŠ¡æˆåŠŸ
                if self.status_writer:
                    self.status_writer.on_task_success(queue_id, task_idx, total_tasks, gpu_ids[0])
                    # æ›´æ–°è¿›ç¨‹çº§çŠ¶æ€
                    from datetime import datetime
                    self.status_writer.update_process_status(
                        queue_id, task_idx,
                        status="completed",
                        current_gpu=None,
                        gpus=[],
                        finished_at=datetime.now().isoformat()
                    )
                return True
            
            # ä»»åŠ¡å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•
            if task.status == "pending":
                logging.info(f"ğŸ”„ Queue {queue_id}: Task {task_idx+1}/{total_tasks} will retry (count={task.retry_count})")
                # æ›´æ–°çŠ¶æ€ï¼šä»»åŠ¡å¤±è´¥ä½†ä¼šé‡è¯•
                if self.status_writer:
                    self.status_writer.on_task_fail(queue_id, task_idx, total_tasks, gpu_ids[0], task.error_type, will_retry=True)
                    # æ›´æ–°è¿›ç¨‹çº§çŠ¶æ€
                    self.status_writer.update_process_status(
                        queue_id, task_idx,
                        status="retrying",
                        current_gpu=None,
                        gpus=[],
                        retry_count=task.retry_count,
                        last_error=task.error_type
                    )
                time.sleep(5)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
            else:
                # ä»»åŠ¡çŠ¶æ€ä¸æ˜¯ pendingï¼Œè¯´æ˜ä¸åº”è¯¥é‡è¯•
                # æ›´æ–°çŠ¶æ€ï¼šä»»åŠ¡å¤±è´¥ä¸”ä¸ä¼šé‡è¯•
                if self.status_writer:
                    self.status_writer.on_task_fail(queue_id, task_idx, total_tasks, gpu_ids[0], task.error_type, will_retry=False)
                    # æ›´æ–°è¿›ç¨‹çº§çŠ¶æ€
                    from datetime import datetime
                    self.status_writer.update_process_status(
                        queue_id, task_idx,
                        status="failed",
                        current_gpu=None,
                        gpus=[],
                        last_error=task.error_type,
                        finished_at=datetime.now().isoformat()
                    )
                return False
        
        logging.error(f"âŒ Queue {queue_id}: Task {task_idx+1}/{total_tasks} exceeded max retries ({max_total_retries})")
        return False
    
    def _wait_for_gpus(self, gpu_count: int, required_memory: int, queue_id: int, timeout: int = 3600) -> Optional[List[int]]:
        """ç­‰å¾…å¯ç”¨çš„å¤šä¸ª GPU å¹¶ç«‹å³æ ‡è®°ä¸ºå ç”¨
        
        Args:
            gpu_count: éœ€è¦çš„ GPU æ•°é‡
            required_memory: æ¯å¼  GPU éœ€è¦çš„æ˜¾å­˜ (GB)
            queue_id: è¯·æ±‚GPUçš„é˜Ÿåˆ—ID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
            
        Returns:
            å¯ç”¨çš„ GPU ID åˆ—è¡¨ï¼Œå¦‚æœè¶…æ—¶è¿”å› None
        """
        start_time = time.time()
        last_log_time = 0
        
        while time.time() - start_time < timeout:
            if not self.running:
                return None
            
            with self.gpu_lock:
                gpu_ids = self.find_available_gpus(gpu_count, required_memory, queue_id)
                if gpu_ids is not None:
                    # ç«‹å³æ ‡è®°ä¸ºå ç”¨ï¼Œé˜²æ­¢å…¶ä»–é˜Ÿåˆ—æŠ¢å 
                    for gpu_id in gpu_ids:
                        self.occupied_gpus[gpu_id] = queue_id
                    logging.info(f"ğŸ”’ GPUs {gpu_ids} acquired by queue {queue_id}")
                    return gpu_ids
            
            # æ²¡æœ‰è¶³å¤Ÿçš„å¯ç”¨ GPUï¼Œæ¯check_timeç§’è¾“å‡ºä¸€æ¬¡ç­‰å¾…æ—¥å¿—
            elapsed = time.time() - start_time
            if time.time() - last_log_time >= check_time:
                # åŠ¨æ€é¢„ç•™çŠ¶æ€
                current_used = self._get_current_user_gpu_count()
                max_allowed = self._get_max_allowed_gpus()
                
                # æ£€æŸ¥æ‰€æœ‰GPUçš„çŠ¶æ€ï¼Œè¾“å‡ºè¯¦ç»†ä¿¡æ¯
                gpu_status = []
                for gpu_id in self.gpus:
                    available = GPUMonitor.get_available_memory(gpu_id)
                    is_occupied = gpu_id in self.occupied_gpus
                    user_procs = GPUMonitor.get_user_processes_on_gpu(gpu_id) if not maximize_resource_utilization else []
                    status = "ğŸ”´" if (is_occupied or user_procs) else "ğŸŸ¢"
                    gpu_status.append(f"GPU{gpu_id}: {status} ({available:.1f}GB)")
                
                logging.info(
                    f"â³ Queue {queue_id}: Waiting for {gpu_count} GPUs ({required_memory}GB each, "
                    f"using {current_used}/{max_allowed} GPUs, elapsed {elapsed:.0f}s) - {' | '.join(gpu_status)}"
                )
                last_log_time = time.time()
            
            # ç­‰å¾…åé‡è¯•
            time.sleep(check_time)
        
        logging.warning(f"â° Timeout waiting for {gpu_count} GPUs with {required_memory}GB memory each")
        return None
    
    def init_status_writer(self, config_index: int = 0):
        """åˆå§‹åŒ–çŠ¶æ€å†™å…¥å™¨"""
        status_dir = os.path.join(SCRIPT_DIR, 'logs', 'status')
        self.status_writer = StatusWriter(
            status_dir=status_dir,
            mode="multi",
            config_index=config_index,
            config_file=commands_path
        )
        
        # è®¾ç½® GPU ä¿¡æ¯
        all_gpus = GPUMonitor.detect_gpus() if use_all_gpus else compete_gpus
        self.status_writer.set_gpus(self.gpus, all_gpus)
        
        # åˆå§‹åŒ–é˜Ÿåˆ—ä¿¡æ¯
        queue_task_counts = {qid: len(tasks) for qid, tasks in self.queues.items()}
        self.status_writer.init_queues(queue_task_counts)
        
        # åˆå§‹åŒ–æ¯ä¸ªé˜Ÿåˆ—çš„è¿›ç¨‹ä¿¡æ¯
        for qid, tasks in self.queues.items():
            processes = []
            for task in tasks:
                processes.append({
                    "commands": task.commands,
                    "memory_gb": task.estimated_memory_gb,
                    "gpu_count": task.gpu_count
                })
            self.status_writer.init_queue_processes(qid, processes)
        
        # è®¾ç½®è¿è¡ŒçŠ¶æ€
        self.status_writer.set_state("running")
    
    def run(self):
        """ä¸»è°ƒåº¦å¾ªç¯ï¼ˆå¤šGPUç‰ˆæœ¬ï¼‰
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        - é˜Ÿå†…ä¸²è¡Œï¼šæ¯ä¸ªé˜Ÿåˆ—çš„ä»»åŠ¡æŒ‰é¡ºåºæ‰§è¡Œ
        - é˜Ÿé—´å¹¶è¡Œï¼šä¸åŒé˜Ÿåˆ—ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        """
        logging.info("ğŸ Starting Multi-GPU competition scheduler")
        self.print_status()
        
        if not self.queues:
            logging.warning("âš ï¸ No queues to process")
            return
        
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰é˜Ÿåˆ—
            max_workers = min(len(self.queues), len(self.gpus))
            logging.info(f"ğŸ”§ Starting {len(self.queues)} queues with {max_workers} workers")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰é˜Ÿåˆ—ä»»åŠ¡
                futures = {}
                for queue_id in self.queues.keys():
                    future = executor.submit(self._run_queue, queue_id)
                    futures[future] = queue_id
                    self.queue_futures[queue_id] = future
                
                # ç­‰å¾…æ‰€æœ‰é˜Ÿåˆ—å®Œæˆ
                for future in concurrent.futures.as_completed(futures):
                    queue_id = futures[future]
                    try:
                        future.result()
                        logging.info(f"âœ… Queue {queue_id} completed")
                    except Exception as e:
                        logging.error(f"âŒ Queue {queue_id} failed with exception: {e}")
            
            # æ‰“å°æœ€ç»ˆçŠ¶æ€
            self.print_status()
            logging.info("ğŸ‰ All queues finished!")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
            if self.status_writer:
                self.status_writer.set_state("completed")
            
        except KeyboardInterrupt:
            logging.info("ğŸ›‘ Received interrupt signal, stopping...")
            self.running = False
            if self.status_writer:
                self.status_writer.set_state("stopping")
        except Exception as e:
            logging.error(f"âŒ Scheduler error: {e}")
            if self.status_writer:
                self.status_writer.set_error(str(e))
                self.status_writer.set_state("failed")
            raise


def main():
    """ä¸»å‡½æ•°"""
    competitor = MultiGPUCompetitor()
    competitor.init_status_writer(config_index=args.config_index)
    competitor.run()


if __name__ == "__main__":
    main()